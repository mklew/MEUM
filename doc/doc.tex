\documentclass[12pt, a4paper]{article}
\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage[polish]{babel}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage[numbers]{natbib}
\title{\textbf{Implementacja i testowanie algorytmu ewolucyjnego, w którym funkcja celu jest 
aproksymowana modelem zastępczym}}
\author{Piotr Jarosik \\ Marek Lewandowski}
\date{}

\begin{document}
\maketitle

\section{Zadanie}
Implementacja i testowanie algorytmu ewolucyjnego, w którym funkcja celu jest aproksymowana modelem 
zastępczym. Na ten model proponuję IBHM, MLP lub model jądrowy.

\section{Aproksymator}
Zgodnie z założeniami wstępnymi użyliśmy perceptronu wielowarstwowego do 
budowy modelu zastępczego - aproksymatora funkcji celu. 

Skorzystaliśmy z gotowego pakietu języka R \emph{neuralnet}, który pozwala na budowę 
sieci neuronowej o dowolnej liczbie warstw ukrytych, liczbie ukrytych neuronów, wybór funkcji aktywacji, 
wybór metody optymalizacji i innych parametrów.

Użyliśmy sieci o następujących parametrach:
\begin{itemize}
\item sieć składa się z 3 warstw, w tym jednej warstwy ukrytej,
\item sieć posiada tylko jeden neuron wyjściowy,
\item wyjście sieci jest liniowe,
\item funkcja aktywacji to tangens hiperboliczny,
\item miarą błędu na zbiorze trenującym jest błąd średniokwadratowy,
\item metoda optymalizacji to RPROP+\cite{Riedmiller93adirect}
\item rozmiar wejścia sieci jest parametryzowany w zależności od aktualnie badanej funkcji i jej wymiaru,
\item liczba neuronów warstwy ukrytej jest parametryzowana podczas badań.
\end{itemize}

Ponadto zastosowaliśmy metodę normalizacji danych przed rozpoczęciem trenowania sieci w celu przyspieszenia algorytmu. Podczas predykcji dane są oczywiście normalizowane, a wynik denormalizowany.

\nocite{*}
\bibliographystyle{plainnat}
\bibliography{bibliography}
\end{document}

