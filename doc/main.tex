\documentclass[runningheads]{llncs}
\usepackage{polski}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\usepackage[polish]{babel}
%\usepackage{geometry}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage[numbers]{natbib}
\usepackage{rotating}
\usepackage{verbatim}
\pagestyle{headings}  
\addtocmark{Algorytm ewolucyjny z funkcją celu aproksymowaną modelem 
zastępczym} 
%
%
%-%-% POCZĄTEK ARTYKUŁU
%
\title{Algorytm ewolucyjny z funkcją celu aproksymowaną modelem zastępczym}
% 
\titlerunning{Algorytm ewolucyjny z funkcją celu aproksymowaną modelem 
zastępczym}   
%
\author{Piotr Jarosik\\Marek Lewandowski}
%
\authorrunning{Piotr Jarosik, Marek Lewandowski} 
%
%
\institute{Wydział Elektroniki i~Technik Informacyjnych\\
Politechnika Warszawska\\
\email{P.Jarosik@stud.elka.pw.edu.pl\\
	M.M.Lewandowski@stud.elka.pw.edu.pl}}

\begin{document}
\maketitle

\section{Zadanie}
Implementacja i testowanie algorytmu ewolucyjnego, w którym funkcja celu 
jest aproksymowana modelem zastępczym. Na ten model proponuję IBHM, MLP lub 
model jądrowy.

\section{Interpretacja}

Celem zadania było zaimplementowanie i przetestowanie algorytmu ewolucyjnego, 
który do optymalizacji wykorzystuje model zastępczy zamiast rzeczywistej 
funkcji celu (która może być \emph{kosztowna obliczeniowo}). Rozwiązanie składa 
się z następujących kroków:
\begin{enumerate}
 \item Budowany jest aproksymator dla pewnej liczby punktów obliczonych dla 
rzeczywistej funkcji celu;
 \item Zbudowany w poprzednim punkcie aproksymator jest wykorzystywany do 
zadania optymalizacji -- poszukiwane jest optimum zbudowanego aproksymatora;
 \item Na podstawie wyników dobierany zostanie kolejny zbiór punktów, dla 
których następnie liczone są wartości rzeczywistej funkcji celu.
\end{enumerate}

Kroki te wykonywane są w pętli tak długo, aż osiągnięty zostanie odpowiedni 
poziom dokładności (zdefiniowany w kolejnych punktach) lub pewna ustalona 
maksymalna liczba iteracji.

\section{Opracowane rozwiązanie}

Rozwiązanie zostało opracowane zgodnie z podaną interpretacją projektu. 
Przyjęto następujące założenia:
\begin{itemize}
 \item jako model zastępczy zastosowano wielowarstwowy perceptron (MLP);
 \item optymalizatorem funkcji celu jest algorytm ewolucyjny (zgodnie z treścią 
zadania).
\end{itemize}
Uszczegółowienia wymaga jeszcze sposób doboru punktów w pierwszej i kolejnych 
iteracjach opracowanego algorytmu. 

W pierwszej iteracji budowane jest \emph{ogólne spojrzenie na rzeczywistą 
funkcję celu} -- aproksymator jest budowany na podstawie punktów wylosowanych z 
dziedziny z rozkładem jednostajnym. Pozwala to 
na określenie potencjalnych miejsc, gdzie być może znajduje się optimum 
rzeczywistej funkcji celu -- dokonywana jest \textbf{eksploracja} dziedziny. 
Jako granice rozkładu jednostajnego przyjęto ograniczenia kostki 
nałożone na daną funkcję (badana jest cała dopuszczalna dziedzina).

W kolejnych iteracjach szczegółowo badane są miejsca, w których być może 
znajduje się optimum globalne funkcji celu -- aproksymator budowany jest na 
podstawie sumy zbioru punktów wylosowanych do tej pory oraz zbioru punktów 
wylosowanych w otoczeniu optimum globalnego znalezionego w poprzedniej 
iteracji. W ten sposób szczegółowo zbadany został obszar potencjalnie 
zawierający optimum globalne rzeczywistej funkcji (\textbf{eksploatacja}) - 
dzięki temu możliwa jest korekta otrzymanego optimum i być może zmiana 
obszaru dziedziny, z którego będą losowane punkty w kolejnych iteracjach (gdy 
np. dotychczas znalezione optimum globalne, po zwiększeniu liczby punktów w 
jego otoczeniu, okaże się być tylko optimum lokalnym). Punkty w trakcie 
eksploatacji są losowane z rozkładem normalnym o wartości oczekiwanej równej 
znalezionemu optimum i macierzy kowariancji dobranej do ograniczeń rzeczywistej 
funkcji celu (tak, aby poszukiwanie było dokonywane głównie wokół wartości 
oczekiwanej, z uwzględnieniem punktów możliwych wokół znalezionego optimum).

Algorytm kończy działanie, gdy:
\begin{itemize}
 \item osiągnięto dobrą aproksymację rzeczywistej funkcji celu -- błąd 
średniokwadratowy nie uległ poprawie przez pewną ustaloną liczbę iteracji;
 \item przekroczono maksymalną liczbę iteracji.
\end{itemize}

Poniżej przedstawiono szczegóły zaimplementowanego algorytmu ewolucyjnego oraz 
perceptronu wielowarstwowego.

\subsection{Algorytm ewolucyjny}

Algorytm ewolucyjny został zaimplementowany zgodnie ze schematem 
zaprezentowanym w \cite{arabas}. Unkonkretyzowania 
wymagają jednak poszczególne operatory, inicjacja populacji oraz kryterium 
zatrzymania. 

\subsubsection{Inicjacja}

Populacja początkowa algorytmu ewolucyjnego wybierana jest z rozkładem 
normalnym o niskiej wariancji (w ramach projektu przyjęto arbitralnie 
wartość $0.1$) wokół punktu początkowego. Punkt początkowy stanowi parametr 
algorytmu \footnote{W rzeczywistości punkt początkowy często jest podawany wraz 
z funkcją celu. Tak jest w przypadku benchmarka BBOB13 -- jednym z 
parametrów wejściowych algorytmu optymalizacji jest punkt początkowy 
proponowany przez benchmark).}

\subsubsection{Selekcja}
W ramach projektu przyjęto \textbf{selekcję turniejową}: w \emph{szranki} 
losowane są punkty z rozkładem jednostajnym z obecnego stanu populacji. 
Następnie reprodukcji (selekcji) podlega najlepszy punkt ze szranki (tj. o 
największej wartości funkcji celu).

\subsubsection{Krzyżowanie}
Opracowany projekt wykorzystuje \textbf{krzyżowanie równomierne}, tj. nowy 
punkt (po skrzyżowaniu) definiowany jest jako:
\begin{equation}
 y = wx_1 + (1-w)x_2
\end{equation}
przy czym $w$ jest wektorem binarnym z jednakowym prawdopodobieństwem jedynek i 
zer. Krzyżowaniu podlegają dokładnie dwa punkty. Krzyżowanie dokonywane jest 
tylko, gdy pewna ustalona zmienna losowa o rozkładzie jednostajnym przekroczy 
wybrany próg -- prawdopodobieństwo krzyżowania. W ramach projektu próg ten 
został ustalony jako $0.5$.

\subsubsection{Mutacja}
Mutacja dokonywana jest z rozkładem normalnym z ustaloną (sparametryzowaną) 
wartością macierzy kowariancji.

\subsubsection{Sukcesja}
W ramach projektu przyjęto \textbf{sukcesję elitarną} -- nowa populacja składa 
się z $\mu$ najlepszych osobników z nowej populacji (wyniku zastosowanych 
powyżej operacji) oraz $k$ najlepszych poprzedniej populacji. W przypadku 
zrealizowanego projektu przyjęto arbitralnie $k=1$.

\subsection{Kryterium zatrzymania}
Jako kryterium zatrzymania algorytmu przyjęto:
\begin{itemize}
 \item brak poprawy jakości rozwiązania przez $\tau$ ostatnich iteracji, tj.
\begin{equation}
 \Phi(X(t-\tau))-\Phi(X(t)) \le \epsilon
\end{equation}
gdzie $\Phi$ -- funkcja celu, $X(t)$ -- najlepszy osobnik znaleziony do chwili 
$t$, $\epsilon$ i $\tau$ -- parametry algorytmu (w ramach projektu przyjęto 
wartości odpowiednio $10$ oraz $0.1$.
\end{itemize}

\subsection{Perceptron wielowarstwowy}

\label{sec:aprox}
Zgodnie z założeniami użyto perceptronu wielowarstwowego do budowy modelu 
zastępczego - aproksymatora funkcji celu. W tym celu, w ramach projektu 
skorzystano z gotowego pakietu języka R \verb neuralnet , który pozwala na 
budowę sieci neuronowej o dowolnej liczbie warstw ukrytych, liczbie ukrytych 
neuronów, wybór funkcji aktywacji, wybór metody optymalizacji i innych 
parametrów.

Użyto sieci o następujących parametrach:
\begin{itemize}
\item sieć składa się z 3 warstw, w tym jednej warstwy ukrytej,
\item sieć posiada tylko jeden neuron wyjściowy,
\item wyjście sieci jest liniowe,
\item funkcja aktywacji to tangens hiperboliczny,
\item miarą błędu na zbiorze trenującym jest błąd średniokwadratowy,
\item metoda optymalizacji to RPROP+
\item rozmiar wejścia sieci jest parametryzowany w zależności od aktualnie 
badanej funkcji i jej wymiaru,
\item liczba neuronów warstwy ukrytej jest parametryzowana podczas badań.
\end{itemize}

Ponadto zastosowano metodę normalizacji danych przed rozpoczęciem trenowania 
sieci w celu przyspieszenia algorytmu. Podczas predykcji dane są normalizowane, 
a wynik denormalizowany.

\subsection{Proces normalizacji}
Próbkę danych $x$ normalizuje się zgodnie ze wzorem:

$x_{norm}=\frac{x - m}{sd}$

gdzie:	$m$ - wartość oczekiwana $x$, $sd$ - standardowe odchylenie $x$.
Wzór podano dla formy wektorowej.

\section{Eksperymenty}
Celem eksperymentów jest zbadanie, jak optymalizacja z wykorzystaniem modelu 
zastępczego wpływa na jakość otrzymywanego rozwiązania. Eksperymenty 
przeprowadzono przy użyciu benchmarku BBOB13. Testy zostały przeprowadzone dla 
funkcji z zakresu $1-24$ dla wymiarów $5D$ oraz $20D$. 

Mając na uwadze charakter problemu, czyli wysoki koszt ewaluacji rzeczywistej 
funkcji, ważnym parametrem oceny rozwiązania jest maksymalna liczba ewaluacji 
funkcji celu FEsmax(fixed cost). Głównym punktem badań była więc analiza liczby 
ewaluacji funkcji niezbędnej do osiągnięcia pewnego poziomu jakości 
rozwiązania. W tym celu, do prezentacji wyników posłużono się łączną 
dystrybuantą empiryczną (ECDF) dla rozkładów liczby ewaluacji funkcji przy 
ustalonym poziomie błędu oraz łączną dystrybuantą empiryczną dla rozkładu 
najlepszych osiągniętych poziomów błędu. Zamieszczone wykresy zostały 
wygenerowane przy użyciu skryptu w języku python opracowanego przez autorów 
benchmarku BBOB13.

Aby mieć pewien punkt odniesienia, opracowane rozwiązanie porównano z 
samym algorytmem ewolucyjnym, który stanowi element opracowanego rozwiązania. 
Parametry, których zmiany zostały badane to: rozmiar utrzymywanej populacji 
($\mu$), rozmiar reprodukowanej populacji ($\lambda$), maksymalna liczba 
iteracji opracowanego algorytmu. Ponadto sprawdzono, jak zmiana liczby 
neuronów na warstwie ukrytej wpływa na jakość opracowanego rozwiązania.

\section{Wyniki}
\newcommand{\rot}[2][2.5]{
  \hspace*{-3.5\baselineskip}%
  \begin{rotate}{90}\hspace{#1em}#2
\end{rotate}}
  
Poniżej przedstawiono wykresy ECDF otrzymane w procedurze eksperymentalnej. Po 
lewej stronie każdej kolumny umieszczone są dystrybuanty rozkładów w dziedzinie 
liczby ewaluacji funkcji celu, zaś w po prawej -- dystrybuanta rozkładu jakości 
(odległości od optimum) dla otrzymywanych wyników. Eksperymenty zostały 
wykonane dla budżetu równego 100.
\newpage  
\subsection{Algorytm ewolucyjny bez modelu zastępczego}
Parametry:
\begin{itemize}
 \item rozmiar utrzymywanej populacji $P^t$: $\mu=100$
 \item rozmiar reprodukowanej populacji $O^t$: $\lambda=100$
 \item maksymalna liczba iteracji algorytmu ewolucyjnego: $10000$
\end{itemize}
\begin{figure*}
\begin{tabular}{l@{\hspace*{-0.025\textwidth}}l@{\hspace*{-0.00\textwidth}}|l@{
\hspace*{-0.025\textwidth}}l}
\multicolumn{2}{c}{$D=5$} & \multicolumn{2}{c}{$D=20$}\\[-0.5ex]
\rot{separable}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_ev/pprldistr_05D_separ} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_ev/ppfvdistr_05D_separ} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_ev/pprldistr_20D_separ} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_ev/ppfvdistr_20D_separ} \\[-2ex]
\rot[1]{moderate}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_ev/pprldistr_05D_lcond} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_ev/ppfvdistr_05D_lcond} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_ev/pprldistr_20D_lcond} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_ev/ppfvdistr_20D_lcond} \\[-2ex]
\rot[1.3]{ill-cond.}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_ev/pprldistr_05D_hcond} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_ev/ppfvdistr_05D_hcond} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_ev/pprldistr_20D_hcond} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_ev/ppfvdistr_20D_hcond} \\[-2ex]
\rot[1.6]{multi-modal}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_ev/pprldistr_05D_multi} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_ev/ppfvdistr_05D_multi} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_ev/pprldistr_20D_multi} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_ev/ppfvdistr_20D_multi} \\[-2ex]
\rot[1.0]{weak struct}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_ev/pprldistr_05D_mult2} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_ev/ppfvdistr_05D_mult2} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_ev/pprldistr_20D_mult2} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_ev/ppfvdistr_20D_mult2}\\[-2ex]
\rot{all}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_ev/pprldistr_05D_noiselessall} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_ev/ppfvdistr_05D_noiselessall} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_ev/pprldistr_20D_noiselessall} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_ev/ppfvdistr_20D_noiselessall}
\vspace*{-0.5ex}
\end{tabular}
\caption{Wykresy ECDF dla algorytmu ewolucyjnego bez modelu zastępczego.}
\end{figure*}
\newpage
\subsection{Algorytm ewolucyjny z wykorzystaniem modelu zastępczego}
Parametry:
\begin{itemize}
 \item rozmiar utrzymywanej populacji $P^t$: $\mu=100$
 \item rozmiar reprodukowanej populacji $O^t$: $\lambda=100$
 \item maksymalna liczba iteracji algorytmu ewolucyjnego: $10000$
 \item liczba neuronów w warstwie ukrytej: $10$
\end{itemize}
\begin{figure*}[!h]
\begin{tabular}{l@{\hspace*{-0.025\textwidth}}l@{\hspace*{-0.00\textwidth}}|l@{
\hspace*{-0.025\textwidth}}l}
\multicolumn{2}{c}{$D=5$} & \multicolumn{2}{c}{$D=20$}\\[-0.5ex]
\rot{separable}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_normal/pprldistr_05D_separ} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_normal/ppfvdistr_05D_separ} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_normal/pprldistr_20D_separ} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_normal/ppfvdistr_20D_separ} \\[-2ex]
\rot[1]{moderate}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_normal/pprldistr_05D_lcond} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_normal/ppfvdistr_05D_lcond} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_normal/pprldistr_20D_lcond} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_normal/ppfvdistr_20D_lcond} \\[-2ex]
\rot[1.3]{ill-cond.}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_normal/pprldistr_05D_hcond} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_normal/ppfvdistr_05D_hcond} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_normal/pprldistr_20D_hcond} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_normal/ppfvdistr_20D_hcond} \\[-2ex]
\rot[1.6]{multi-modal}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_normal/pprldistr_05D_multi} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_normal/ppfvdistr_05D_multi} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_normal/pprldistr_20D_multi} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_normal/ppfvdistr_20D_multi} \\[-2ex]
\rot[1.0]{weak struct}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_normal/pprldistr_05D_mult2} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_normal/ppfvdistr_05D_mult2} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_normal/pprldistr_20D_mult2} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_normal/ppfvdistr_20D_mult2}\\[-2ex]
\rot{all}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_normal/pprldistr_05D_noiselessall} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_normal/ppfvdistr_05D_noiselessall} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_normal/pprldistr_20D_noiselessall} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_normal/ppfvdistr_20D_noiselessall}
\vspace*{-0.5ex}
\end{tabular}
\caption{Wykresy ECDF dla rozwiązania z modelem zastępczym (perceptron 
wielowarstwowy z 10 neuronami w warstwie ukrytej).}
\end{figure*}

\begin{comment}
\begin{figure*}
\begin{tabular}{l@{\hspace*{-0.025\textwidth}}l@{\hspace*{-0.00\textwidth}}|l@{
\hspace*{-0.025\textwidth}}l}
\multicolumn{2}{c}{$D=5$} & \multicolumn{2}{c}{$D=20$}\\[-0.5ex]
\rot{separable}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_unit50/pprldistr_05D_separ} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_unit50/ppfvdistr_05D_separ} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_unit50/pprldistr_20D_separ} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_unit50/ppfvdistr_20D_separ} \\[-2ex]
\rot[1]{moderate}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_unit50/pprldistr_05D_lcond} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_unit50/ppfvdistr_05D_lcond} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_unit50/pprldistr_20D_lcond} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_unit50/ppfvdistr_20D_lcond} \\[-2ex]
\rot[1.3]{ill-cond.}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_unit50/pprldistr_05D_hcond} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_unit50/ppfvdistr_05D_hcond} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_unit50/pprldistr_20D_hcond} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_unit50/ppfvdistr_20D_hcond} \\[-2ex]
\rot[1.6]{multi-modal}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_unit50/pprldistr_05D_multi} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_unit50/ppfvdistr_05D_multi} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_unit50/pprldistr_20D_multi} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_unit50/ppfvdistr_20D_multi} \\[-2ex]
\rot[1.0]{weak struct}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_unit50/pprldistr_05D_mult2} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_unit50/ppfvdistr_05D_mult2} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_unit50/pprldistr_20D_mult2} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_unit50/ppfvdistr_20D_mult2}\\[-2ex]
\rot{all}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_unit50/pprldistr_05D_noiselessall} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_unit50/ppfvdistr_05D_noiselessall} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_unit50/pprldistr_20D_noiselessall} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_unit50/ppfvdistr_20D_noiselessall}
\vspace*{-0.5ex}
\end{tabular}
\end{figure*}
\end{comment}
\newpage
\subsection{Algorytm ewolucyjny z wykorzystaniem modelu zastępczego}

Parametry:
\begin{itemize}
 \item rozmiar utrzymywanej populacji $P^t$: $\mu=250$
 \item rozmiar reprodukowanej populacji $O^t$: $\lambda=250$
 \item maksymalna liczba iteracji algorytmu ewolucyjnego: $10000$
 \item liczba neuronów w warstwie ukrytej: $50$
\end{itemize}
\begin{figure*}[!h]
\begin{tabular}{l@{\hspace*{-0.025\textwidth}}l@{\hspace*{-0.00\textwidth}}|l@{
\hspace*{-0.025\textwidth}}l}
\multicolumn{2}{c}{$D=5$} & \multicolumn{2}{c}{$D=20$}\\[-0.5ex]
\rot{separable}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_lambda25_mu20/pprldistr_05D_separ} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_lambda25_mu20/ppfvdistr_05D_separ} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_lambda25_mu20/pprldistr_20D_separ} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_lambda25_mu20/ppfvdistr_20D_separ} \\[-2ex]
\rot[1]{moderate}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_lambda25_mu20/pprldistr_05D_lcond} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_lambda25_mu20/ppfvdistr_05D_lcond} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_lambda25_mu20/pprldistr_20D_lcond} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_lambda25_mu20/ppfvdistr_20D_lcond} \\[-2ex]
\rot[1.3]{ill-cond.}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_lambda25_mu20/pprldistr_05D_hcond} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_lambda25_mu20/ppfvdistr_05D_hcond} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_lambda25_mu20/pprldistr_20D_hcond} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_lambda25_mu20/ppfvdistr_20D_hcond} \\[-2ex]
\rot[1.6]{multi-modal}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_lambda25_mu20/pprldistr_05D_multi} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_lambda25_mu20/ppfvdistr_05D_multi} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_lambda25_mu20/pprldistr_20D_multi} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_lambda25_mu20/ppfvdistr_20D_multi} \\[-2ex]
\rot[1.0]{weak struct}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_lambda25_mu20/pprldistr_05D_mult2} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_lambda25_mu20/ppfvdistr_05D_mult2} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_lambda25_mu20/pprldistr_20D_mult2} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_lambda25_mu20/ppfvdistr_20D_mult2}\\[-2ex]
\rot{all}
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_lambda25_mu20/pprldistr_05D_noiselessall} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_lambda25_mu20/ppfvdistr_05D_noiselessall} &
\includegraphics[width=0.268\textwidth,trim=0 0 0 13mm, 
clip]{ppdata_lambda25_mu20/pprldistr_20D_noiselessall} &
\includegraphics[width=0.2362\textwidth,trim=2.40cm 0 0 13mm, 
clip]{ppdata_lambda25_mu20/ppfvdistr_20D_noiselessall}
\vspace*{-0.5ex}
\end{tabular}
\caption{Wykresy ECDF dla rozwiązania z modelem zastępczym (perceptron 
wielowarstwowy z 50 neuronami w warstwie ukrytej).}
\end{figure*}

\section{Dyskusja wyników}

Dla algorytmu ewolucyjnego, który nie wykorzystuje żadnego modelu zastępczego, 
wymagana minimalna odległość do optimum lokalnego osiągalne jest dopiero przy 
dość dużej liczbie ewaluacji funkcji celu. W przypadku najtrudniejszej w 
osiągnięciu wartości docelowej, tj. dla $f_{optim} + 10^{-8}$ (zaznaczone 
czerwoną linią) dla funkcji separowalnych o pięciu wymiarach wymagane jest co 
najmniej $10^3/DIM$ ewaluacji funkcji celu. Sytuacja ma się oczywiście jeszcze 
gorzej dla 20 wymiarów -- dla żadnej z funkcji z $f1-24$ praktycznie nie jest 
osiągalny najtrudniejszy przypadek. Analizując ECDF dla $\Delta f$ (wykresy po 
prawej stronie) można zauważyć, iż dla małej liczby ewaluacji funkcji celu 
(oznaczone kolorem seledynowym) odległość od wartości optymalnej $\Delta f$ 
jest dość wysoka (np. wartość dystrybuanty osiąga wartości większe od zera 
blisko wartości równej 100). 

Dla algorytmu ewolucyjnego z modelem zastępczym w postaci perceptronu 
wielowarstwowego (z 10 neuronami na warstwie ukrytej) liczba ewaluacji funkcji 
do osiągnięcia najprostszego przypadku jest relatywnie niska (w porównaniu z 
algorytmem ewolucyjnym bez specjalnego zabiegu). Na przykład, dla funkcji 
$f1-5$ najtrudniejsza odległość od optimum do osiągnięcia (oznaczony czerwoną 
linią) jest osiągalna w próbach już z mniej niż 100 ewaluacji funkcji celu. 
Wykresy dystrybuant ECDF dla $\Delta f$ nie różnią się znacząco 
od dystrybuant dla prostego algorytmu ewolucyjnego, aczkolwiek dla małej liczby 
ewaluacji funkcji celu (seledynowy kolor) duża część uruchomień skutkuje dość 
niską $\Delta f$.

Zwiększenie liczby neuronów w ukrytej warstwie do 50 oraz zwiększenie liczności 
populacji nie objawia się znaczącą poprawą rozwiązania. Dystrybuanty dla tak 
dobranych parametrów algorytmu są bardzo zbliżone do dystrybuant dla modelu z 
10 neuronami na warstwie ukrytej, w niektórych przypadkach można zauważyć 
delikatne przesunięcie wykresów dystrybuant ku co raz mniejszym wartościom 
$\Delta f$. 
\section{Wnioski}

Zastosowanie modelu zastępczego pozwoliło w znaczący sposób ograniczyć liczbę 
ewaluacji funkcji niezbędnych do osiągnięcia określonej dokładności 
rozwiązania. Wniosek ten jest dość oczywisty -- w zaimplementowanym projekcie 
zastosowano model zastępczy, który to jest głównie eksploatowany do określenia 
wartości funkcji celu dla punktów z populacji przez algorytm ewolucyjny. 
Ewaluacja oryginalnej funkcji celu została ograniczona tylko do pewnej małej, 
sparametryzowanej wartości, która mówi, jak wiele punktów będzie użytych do 
poprawy aproksymacji przez model zastępczy. 
Zastosowanie modelu zastępczego ma więc jeszcze jedną istotną zaletę -- pozwala 
w prostszy sposób (np. tak jak już wcześniej wspomniano -- poprzez 
parametryzację) na zarządzanie tym, jak bardzo prawdziwa funkcja celu jest 
wykorzystywana (\emph{eksploatowana}) w zaimplementowanym algorytmie. Fakt ten 
może być szczególnie istotny w praktycznych zastosowaniach, kiedy użytkownik 
chce kontrolować w prosty sposób wykorzystanie pewnej (być może) kosztownej 
operacji.

Przejście z prostego algorytmu ewolucyjnego do rozwiązania z modelem zastępczym 
pozwoliło ograniczyć liczbę ewaluacji funkcji, aczkolwiek dalsza zmiana 
parametrów algorytmu już niekoniecznie. Innym, możliwie dobry pomysłem mogłoby 
być manipulowanie nie tylko liczbą neuronów w warstwie ukrytej, ale również 
innymi, niemodyfikowanymi w ramach tego projektu parametrami. Dobrym pomysłem 
mogłoby być wykorzystanie gradientu funkcji do odpowiedniego przygotowania 
macierzy kowariancji rozkładu normalnego, z którego losowane są punkty w 
trakcie eksploatacji badanej funkcji celu. 

\begin{thebibliography}{5}
\bibitem{arabas}
Jarosław Arabas, Wykłady z algorytmów ewolucyjnych,  Wydawnictwo 
Naukowo-Techniczne Warszawa 2004 
\end{thebibliography}
\end{document}

